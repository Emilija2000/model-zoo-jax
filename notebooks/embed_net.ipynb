{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From Neel's code\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Callable\n",
    "import json\n",
    "import functools\n",
    "import math\n",
    "\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random, nn\n",
    "\n",
    "from optax import adam, rmsprop, sgd\n",
    "\n",
    "import haiku as hk\n",
    "from haiku.initializers import Initializer, Constant, RandomNormal, TruncatedNormal, VarianceScaling\n",
    "\n",
    "\n",
    "def ctc_net_fn(x: jnp.ndarray,\n",
    "               n_classes: int,\n",
    "               n_conv_layers: int = 3,\n",
    "               kernel_size: tuple = (3, 3),\n",
    "               n_filters: int = 32,\n",
    "               n_fc_layers: int = 3,\n",
    "               fc_width: int = 128,\n",
    "               activation: Callable = nn.relu,\n",
    "               w_init: Initializer = TruncatedNormal()) -> jnp.ndarray:  # TODO: Batchnorm?\n",
    "    convs = [hk.Conv2D(output_channels=n_filters, kernel_shape=kernel_size, padding=\"SAME\", w_init=w_init)\n",
    "             for _ in range(n_conv_layers)]\n",
    "    fcs = [hk.Linear(fc_width, w_init=w_init) for _ in range(n_fc_layers - 1)]\n",
    "\n",
    "    seq = []\n",
    "    for conv in convs:\n",
    "        seq.append(conv)\n",
    "        seq.append(activation)\n",
    "    seq.append(hk.Flatten())\n",
    "    for fc in fcs:\n",
    "        seq.append(fc)\n",
    "        seq.append(activation)\n",
    "    seq.append(hk.Linear(n_classes, w_init=w_init))\n",
    "\n",
    "    net = hk.Sequential(seq)\n",
    "    return net(x)\n",
    "\n",
    "\n",
    "key = random.PRNGKey(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "from meta_transformer import utils\n",
    "from jax import vmap\n",
    "from meta_transformer.transformer import Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = hk.without_apply_rng(hk.transform(ctc_net_fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conv2_d': {'b': (32,), 'w': (3, 3, 1, 32)},\n",
       " 'conv2_d_1': {'b': (32,), 'w': (3, 3, 32, 32)},\n",
       " 'conv2_d_2': {'b': (32,), 'w': (3, 3, 32, 32)},\n",
       " 'linear': {'b': (128,), 'w': (32768, 128)},\n",
       " 'linear_1': {'b': (128,), 'w': (128, 128)},\n",
       " 'linear_2': {'b': (10,), 'w': (128, 10)}}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key, subkey = random.split(key)\n",
    "params = net.init(subkey, jnp.ones((1, 32, 32, 1)), 10)\n",
    "jax.tree_util.tree_map(lambda x: x.shape, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conv2_d': {'b': 32, 'w': 288},\n",
       " 'conv2_d_1': {'b': 32, 'w': 9216},\n",
       " 'conv2_d_2': {'b': 32, 'w': 9216},\n",
       " 'linear': {'b': 128, 'w': 4194304},\n",
       " 'linear_1': {'b': 128, 'w': 16384},\n",
       " 'linear_2': {'b': 10, 'w': 1280}}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.tree_util.tree_map(lambda x: x.size, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.23105"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils.count_params(params) / 1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_weights(weights: jnp.ndarray, chunk_size: int) -> jnp.ndarray:\n",
    "    flat_weights = weights.flatten()\n",
    "    flat_weights = utils.pad_to_chunk_size(flat_weights, chunk_size)\n",
    "    weight_chunks = jnp.split(flat_weights, len(flat_weights) // chunk_size)\n",
    "    return jnp.array(weight_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclasses.dataclass\n",
    "class WeightEmbedding(hk.Module):\n",
    "    \"\"\"A module that embeds an array of neural network weights\"\"\"\n",
    "    chunk_size: int\n",
    "    embed_dim: int\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        weights: jax.Array,\n",
    "    ) -> jax.Array:  # [B, T, D]\n",
    "        embed = hk.Linear(self.embed_dim)\n",
    "        weight_chunks = hk.vmap(chunk_weights, (0, None))(weights, self.chunk_size)  # [B, T, D]\n",
    "        embeddings = embed(weight_chunks)\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class NetEmbedding(hk.Module):\n",
    "    \"\"\"A module that creates embedding vectors from neural network params.\"\"\"\n",
    "    embed_dim: int\n",
    "\n",
    "    def __call__(\n",
    "            self,\n",
    "            params: dict,\n",
    "    ) -> jax.Array:\n",
    "        conv_embed = WeightEmbedding(chunk_size=256, embed_dim=self.embed_dim)\n",
    "        linear_embed = WeightEmbedding(chunk_size=1024, embed_dim=self.embed_dim)\n",
    "        bias_embed = WeightEmbedding(chunk_size=16, embed_dim=self.embed_dim)\n",
    "\n",
    "        params_dict = {f\"{k}/{subk}\": subv for k, v in params.items() \n",
    "                    for subk, subv in v.items()}\n",
    "\n",
    "        embeddings = []\n",
    "        for k, v in params_dict.items():\n",
    "            if k.endswith('b'):\n",
    "                embeddings.append(bias_embed(v))\n",
    "            else:\n",
    "                embeddings.append(conv_embed(v) if 'conv' in k else linear_embed(v))\n",
    "        return jnp.concatenate(embeddings, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_stack(trees):\n",
    "    \"\"\"Stacks a list of trees into a single tree with an extra dimension.\"\"\"\n",
    "    return jax.tree_map(lambda *x: jnp.stack(x), *trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conv2_d': {'b': (2, 32), 'w': (2, 3, 3, 1, 32)},\n",
       " 'conv2_d_1': {'b': (2, 32), 'w': (2, 3, 3, 32, 32)},\n",
       " 'conv2_d_2': {'b': (2, 32), 'w': (2, 3, 3, 32, 32)},\n",
       " 'linear': {'b': (2, 128), 'w': (2, 32768, 128)},\n",
       " 'linear_1': {'b': (2, 128), 'w': (2, 128, 128)},\n",
       " 'linear_2': {'b': (2, 10), 'w': (2, 128, 10)}}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked = tree_stack([params, params])\n",
    "jax.tree_map(lambda x: x.shape, stacked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_fn(params: dict):\n",
    "    net_embedding = NetEmbedding(embed_dim=32)\n",
    "    return net_embedding(params)\n",
    "\n",
    "test = hk.without_apply_rng(hk.transform(test_fn))\n",
    "key, subkey = random.split(key)\n",
    "meta_params = test.init(subkey, stacked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.041568"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils.count_params(meta_params) / 1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = jax.jit(functools.partial(test.apply, meta_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 4211, 32)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_embeds = embed(stacked)\n",
    "param_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclasses.dataclass\n",
    "class Classifier(hk.Module):\n",
    "  \"\"\"A ViT-style classifier.\"\"\"\n",
    "\n",
    "  transformer: Transformer\n",
    "  model_size: int\n",
    "  num_classes: int\n",
    "  name: Optional[str] = None\n",
    "  chunk_size: Optional[int] = 4\n",
    "\n",
    "  def __call__(\n",
    "      self,\n",
    "      params: dict,\n",
    "      *,\n",
    "      is_training: bool = True,\n",
    "  ) -> jax.Array:\n",
    "    \"\"\"Forward pass. Returns a sequence of logits.\"\"\"\n",
    "    net_embed = NetEmbedding(embed_dim=self.model_size)\n",
    "    embeddings = net_embed(params)  # [B, T, D]\n",
    "    _, seq_len, _ = embeddings.shape\n",
    "\n",
    "    # Embed the patches and positions.\n",
    "    positional_embeddings = hk.get_parameter(\n",
    "        'positional_embeddings', [seq_len, self.model_size], init=jnp.zeros)\n",
    "    input_embeddings = embeddings + positional_embeddings  # [B, T, D]\n",
    "\n",
    "    # Run the transformer over the inputs.\n",
    "    embeddings = self.transformer(\n",
    "        input_embeddings,\n",
    "        is_training=is_training,\n",
    "    )  # [B, T, D]\n",
    "\n",
    "    return hk.Linear(self.num_classes)(embeddings)  # [B, T, V]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn(params: dict):\n",
    "    net = Classifier(\n",
    "        model_size=4*32, \n",
    "        num_classes=10, \n",
    "        transformer=Transformer(\n",
    "            num_heads=4,\n",
    "            num_layers=2,\n",
    "            key_size=32,\n",
    "            dropout_rate=0.1,\n",
    "        ))\n",
    "    return net(params)\n",
    "\n",
    "\n",
    "model = hk.transform(model_fn)\n",
    "    \n",
    "key, subkey = random.split(key)\n",
    "meta_params = jax.jit(model.init)(subkey, stacked)\n",
    "model_forward = jax.jit(model.apply)\n",
    "key, subkey = random.split(key)\n",
    "out = model_forward(meta_params, subkey, stacked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 4211, 10)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "meta-models",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
